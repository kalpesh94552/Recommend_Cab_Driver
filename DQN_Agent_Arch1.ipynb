{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Paths\n",
    "timeMatrix_path = \"inputs/TM.npy\"\n",
    "\n",
    "# Importing the environment\n",
    "from references.Env import CabDriver\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Test Libs\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the environment\n",
    "env = CabDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(timeMatrix_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialise Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "States_track = collections.defaultdict(dict)\n",
    "Rewards_track = collections.defaultdict(dict)\n",
    "\n",
    "print(len(States_track))\n",
    "print(len(Rewards_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "def initialise_tracking_states():\n",
    "#     self.track_state = np.array(env.state_encod_arch1([0,7,4])).reshape(1, 36)\n",
    "    sample_states = [(0, 7, 4),(1, 2, 5)]\n",
    "    for state in sample_states:\n",
    "        for action in env.action_space:\n",
    "            States_track[state][action] = []\n",
    "    Rewards_track['Rewards'] = []\n",
    "    Rewards_track['Episode'] = []\n",
    "\n",
    "# Defining a function to track the states initialized\n",
    "def save_tracking_states(state, qval):\n",
    "    if state in States_track.keys():\n",
    "        for i,action in enumerate(States_track[state].keys()):\n",
    "            States_track[state][action].append(qval[0][i])\n",
    "            \n",
    "# Defining a function to track the states initialized\n",
    "def save_total_rewards(total_reward, episode_num):\n",
    "    Rewards_track['Rewards'].append(total_reward)\n",
    "    Rewards_track['Episode'].append(episode_num)\n",
    "\n",
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.h5', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size \n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.02      \n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_decay = 0.0009\n",
    "        self.epsilon_min = 0.001\n",
    "        self.batch_size = 32      \n",
    "        self.epsilon = 0.001\n",
    "        \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(200,activation='relu',input_dim=(self.state_size), kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(150,activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(100,activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation = 'linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Write your code here:\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay *episode)\n",
    "        self.z = np.random.random()  \n",
    "        possible_actions_index, possible_actions = env.requests(state)\n",
    "\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        if self.z > self.epsilon:\n",
    "            #Greedy action\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "            q_value = self.model.predict(state)\n",
    "#             print(\"q_value\")\n",
    "#             print(q_value)\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "#             print(q_vals_possible)\n",
    "            actionIndex = possible_actions_index[np.argmax(q_vals_possible)]\n",
    "            action = env.action_space[actionIndex]\n",
    "            \n",
    "        # Decay in Îµ after we generate each sample from the environment       \n",
    "        else:\n",
    "            #Random\n",
    "            action = random.choice(possible_actions)\n",
    "        return action\n",
    "        \n",
    "    def append_sample(self, state, action, reward, next_state,terminal_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state,terminal_state))\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size: \n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, terminal_states = [], [], []\n",
    "            \n",
    "            #TRAIN THE MODEL\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, terminal_state = mini_batch[i]\n",
    "#                 print(actions)\n",
    "                actions.append(env.action_space.index(action))\n",
    "                rewards.append(reward)\n",
    "                terminal_states.append(terminal_state)\n",
    "#                 print(\"old_next_state\")\n",
    "#                 print(next_state)\n",
    "#                 print(\"new_next_state\")\n",
    "                \n",
    "                # Write your code from here\n",
    "                #1. Update your 'update_output' and 'update_input' batch\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "            \n",
    "            # 2. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)\n",
    "            target_nextqval = self.model.predict(update_output)\n",
    "            \n",
    "            # 3. Get the target for the Q-network\n",
    "            for i in range(self.batch_size):\n",
    "                if terminal_state:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor* np.max(target_nextqval[i])\n",
    "            \n",
    "            # 4. Fit your model and track the loss values\n",
    "#             print(\"update_input\")\n",
    "#             print(update_input[0])\n",
    "#             print(\"target\")\n",
    "            self.model.fit(update_input, target, batch_size = self.batch_size, epochs=1, verbose=0 )\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19 | Reward: 386.0 | Epsilon: 0.98 | Time: 170.13\n",
      "Episode: 39 | Reward: 186.0 | Epsilon: 0.97 | Time: 346.44\n",
      "Episode: 59 | Reward: 53.0 | Epsilon: 0.95 | Time: 520.12\n",
      "Episode: 79 | Reward: 240.0 | Epsilon: 0.93 | Time: 699.52\n",
      "Episode: 99 | Reward: 364.0 | Epsilon: 0.91 | Time: 874.1\n",
      "Episode: 119 | Reward: 306.0 | Epsilon: 0.9 | Time: 1051.53\n",
      "Episode: 139 | Reward: 418.0 | Epsilon: 0.88 | Time: 1225.74\n",
      "Episode: 159 | Reward: -28.0 | Epsilon: 0.87 | Time: 1402.78\n",
      "Episode: 179 | Reward: 276.0 | Epsilon: 0.85 | Time: 1582.9\n",
      "Episode: 199 | Reward: 197.0 | Epsilon: 0.84 | Time: 1766.93\n",
      "Episode: 219 | Reward: 409.0 | Epsilon: 0.82 | Time: 1950.02\n",
      "Episode: 239 | Reward: 249.0 | Epsilon: 0.81 | Time: 2134.14\n",
      "Episode: 259 | Reward: 281.0 | Epsilon: 0.79 | Time: 2316.19\n",
      "Episode: 279 | Reward: 358.0 | Epsilon: 0.78 | Time: 2495.62\n",
      "Episode: 299 | Reward: 370.0 | Epsilon: 0.76 | Time: 2675.44\n",
      "Episode: 319 | Reward: 541.0 | Epsilon: 0.75 | Time: 2853.49\n",
      "Episode: 339 | Reward: 209.0 | Epsilon: 0.74 | Time: 3027.37\n",
      "Episode: 359 | Reward: 500.0 | Epsilon: 0.72 | Time: 3208.06\n",
      "Episode: 379 | Reward: 461.0 | Epsilon: 0.71 | Time: 3383.56\n",
      "Episode: 399 | Reward: 392.0 | Epsilon: 0.7 | Time: 3570.44\n",
      "Episode: 419 | Reward: 632.0 | Epsilon: 0.69 | Time: 3749.2\n",
      "Episode: 439 | Reward: 553.0 | Epsilon: 0.67 | Time: 3935.99\n",
      "Episode: 459 | Reward: 340.0 | Epsilon: 0.66 | Time: 4127.97\n",
      "Episode: 479 | Reward: 384.0 | Epsilon: 0.65 | Time: 4312.0\n",
      "Episode: 499 | Reward: 240.0 | Epsilon: 0.64 | Time: 4500.22\n",
      "Episode: 519 | Reward: 586.0 | Epsilon: 0.63 | Time: 4688.19\n",
      "Episode: 539 | Reward: 350.0 | Epsilon: 0.62 | Time: 4872.31\n",
      "Episode: 559 | Reward: 302.0 | Epsilon: 0.61 | Time: 5053.43\n",
      "Episode: 579 | Reward: 234.0 | Epsilon: 0.59 | Time: 5243.24\n",
      "Episode: 599 | Reward: 521.0 | Epsilon: 0.58 | Time: 5425.12\n",
      "Episode: 619 | Reward: 319.0 | Epsilon: 0.57 | Time: 5612.23\n",
      "Episode: 639 | Reward: 438.0 | Epsilon: 0.56 | Time: 5796.99\n",
      "Episode: 659 | Reward: 238.0 | Epsilon: 0.55 | Time: 5993.41\n",
      "Episode: 679 | Reward: 532.0 | Epsilon: 0.54 | Time: 6179.4\n",
      "Episode: 699 | Reward: 267.0 | Epsilon: 0.53 | Time: 6369.86\n",
      "Episode: 719 | Reward: 619.0 | Epsilon: 0.52 | Time: 6570.94\n",
      "Episode: 739 | Reward: 690.0 | Epsilon: 0.51 | Time: 6758.98\n",
      "Episode: 759 | Reward: 384.0 | Epsilon: 0.51 | Time: 6960.05\n",
      "Episode: 779 | Reward: 616.0 | Epsilon: 0.5 | Time: 7167.08\n",
      "Episode: 799 | Reward: 630.0 | Epsilon: 0.49 | Time: 7358.21\n",
      "Episode: 819 | Reward: 671.0 | Epsilon: 0.48 | Time: 7543.61\n",
      "Episode: 839 | Reward: 883.0 | Epsilon: 0.47 | Time: 7761.19\n",
      "Episode: 859 | Reward: 747.0 | Epsilon: 0.46 | Time: 7974.99\n",
      "Episode: 879 | Reward: 709.0 | Epsilon: 0.45 | Time: 8176.18\n",
      "Episode: 899 | Reward: 825.0 | Epsilon: 0.45 | Time: 8367.46\n",
      "Episode: 919 | Reward: 511.0 | Epsilon: 0.44 | Time: 8566.81\n",
      "Episode: 939 | Reward: 407.0 | Epsilon: 0.43 | Time: 8754.76\n",
      "Episode: 959 | Reward: 472.0 | Epsilon: 0.42 | Time: 8942.31\n",
      "Episode: 979 | Reward: 402.0 | Epsilon: 0.41 | Time: 9140.0\n",
      "Episode: 999 | Reward: 795.0 | Epsilon: 0.41 | Time: 9331.39\n",
      "Episode: 1019 | Reward: 479.0 | Epsilon: 0.4 | Time: 9529.04\n",
      "Episode: 1039 | Reward: 499.0 | Epsilon: 0.39 | Time: 9717.57\n",
      "Episode: 1059 | Reward: 461.0 | Epsilon: 0.39 | Time: 9919.37\n",
      "Episode: 1079 | Reward: 870.0 | Epsilon: 0.38 | Time: 10123.64\n",
      "Episode: 1099 | Reward: 349.0 | Epsilon: 0.37 | Time: 10324.43\n",
      "Episode: 1119 | Reward: 701.0 | Epsilon: 0.37 | Time: 10522.31\n",
      "Episode: 1139 | Reward: 210.0 | Epsilon: 0.36 | Time: 10722.93\n",
      "Episode: 1159 | Reward: 514.0 | Epsilon: 0.35 | Time: 10921.43\n",
      "Episode: 1179 | Reward: 492.0 | Epsilon: 0.35 | Time: 11115.35\n",
      "Episode: 1199 | Reward: 56.0 | Epsilon: 0.34 | Time: 11322.65\n",
      "Episode: 1219 | Reward: 1186.0 | Epsilon: 0.33 | Time: 11533.63\n",
      "Episode: 1239 | Reward: 653.0 | Epsilon: 0.33 | Time: 11727.87\n",
      "Episode: 1259 | Reward: 592.0 | Epsilon: 0.32 | Time: 11937.98\n",
      "Episode: 1279 | Reward: 441.0 | Epsilon: 0.32 | Time: 12139.41\n",
      "Episode: 1299 | Reward: 330.0 | Epsilon: 0.31 | Time: 12359.43\n",
      "Episode: 1319 | Reward: 1118.0 | Epsilon: 0.31 | Time: 12561.88\n",
      "Episode: 1339 | Reward: 493.0 | Epsilon: 0.3 | Time: 12762.2\n",
      "Episode: 1359 | Reward: 598.0 | Epsilon: 0.3 | Time: 12963.25\n",
      "Episode: 1379 | Reward: 203.0 | Epsilon: 0.29 | Time: 13159.78\n",
      "Episode: 1399 | Reward: 640.0 | Epsilon: 0.28 | Time: 13363.52\n",
      "Episode: 1419 | Reward: 1226.0 | Epsilon: 0.28 | Time: 13570.45\n",
      "Episode: 1439 | Reward: 1213.0 | Epsilon: 0.27 | Time: 13788.3\n",
      "Episode: 1459 | Reward: 401.0 | Epsilon: 0.27 | Time: 13993.25\n",
      "Episode: 1479 | Reward: 975.0 | Epsilon: 0.26 | Time: 14209.89\n",
      "Episode: 1499 | Reward: 380.0 | Epsilon: 0.26 | Time: 14417.68\n",
      "Episode: 1519 | Reward: 534.0 | Epsilon: 0.26 | Time: 14618.84\n",
      "Episode: 1539 | Reward: 934.0 | Epsilon: 0.25 | Time: 14824.28\n",
      "Episode: 1559 | Reward: 970.0 | Epsilon: 0.25 | Time: 15029.99\n",
      "Episode: 1579 | Reward: 1156.0 | Epsilon: 0.24 | Time: 15236.42\n",
      "Episode: 1599 | Reward: 974.0 | Epsilon: 0.24 | Time: 15448.97\n",
      "Episode: 1619 | Reward: 501.0 | Epsilon: 0.23 | Time: 15657.9\n",
      "Episode: 1639 | Reward: 322.0 | Epsilon: 0.23 | Time: 15867.76\n",
      "Episode: 1659 | Reward: 435.0 | Epsilon: 0.23 | Time: 16076.37\n",
      "Episode: 1679 | Reward: 761.0 | Epsilon: 0.22 | Time: 16284.5\n",
      "Episode: 1699 | Reward: 324.0 | Epsilon: 0.22 | Time: 16490.95\n",
      "Episode: 1719 | Reward: 419.0 | Epsilon: 0.21 | Time: 16700.33\n",
      "Episode: 1739 | Reward: 745.0 | Epsilon: 0.21 | Time: 16918.74\n",
      "Episode: 1759 | Reward: 1005.0 | Epsilon: 0.21 | Time: 17125.46\n",
      "Episode: 1779 | Reward: 921.0 | Epsilon: 0.2 | Time: 17326.24\n",
      "Episode: 1799 | Reward: 103.0 | Epsilon: 0.2 | Time: 17537.94\n",
      "Episode: 1819 | Reward: 588.0 | Epsilon: 0.2 | Time: 17745.93\n",
      "Episode: 1839 | Reward: 467.0 | Epsilon: 0.19 | Time: 17954.84\n",
      "Episode: 1859 | Reward: 647.0 | Epsilon: 0.19 | Time: 18166.46\n",
      "Episode: 1879 | Reward: 548.0 | Epsilon: 0.19 | Time: 18380.71\n",
      "Episode: 1899 | Reward: 694.0 | Epsilon: 0.18 | Time: 18594.57\n",
      "Episode: 1919 | Reward: 911.0 | Epsilon: 0.18 | Time: 18809.66\n",
      "Episode: 1939 | Reward: 1321.0 | Epsilon: 0.18 | Time: 19033.54\n",
      "Episode: 1959 | Reward: 837.0 | Epsilon: 0.17 | Time: 19247.63\n",
      "Episode: 1979 | Reward: 746.0 | Epsilon: 0.17 | Time: 19468.03\n",
      "Episode: 1999 | Reward: 1150.0 | Epsilon: 0.17 | Time: 19689.39\n",
      "Episode: 2019 | Reward: 1160.0 | Epsilon: 0.16 | Time: 19921.16\n",
      "Episode: 2039 | Reward: 726.0 | Epsilon: 0.16 | Time: 20121.02\n",
      "Episode: 2059 | Reward: 811.0 | Epsilon: 0.16 | Time: 20330.79\n",
      "Episode: 2079 | Reward: 847.0 | Epsilon: 0.15 | Time: 20544.4\n",
      "Episode: 2099 | Reward: 773.0 | Epsilon: 0.15 | Time: 20762.4\n",
      "Episode: 2119 | Reward: 779.0 | Epsilon: 0.15 | Time: 20974.12\n",
      "Episode: 2139 | Reward: 1066.0 | Epsilon: 0.15 | Time: 21181.62\n",
      "Episode: 2159 | Reward: 582.0 | Epsilon: 0.14 | Time: 21424.08\n",
      "Episode: 2179 | Reward: 1064.0 | Epsilon: 0.14 | Time: 21636.68\n",
      "Episode: 2199 | Reward: 1427.0 | Epsilon: 0.14 | Time: 21867.87\n",
      "Episode: 2219 | Reward: 710.0 | Epsilon: 0.14 | Time: 22082.36\n",
      "Episode: 2239 | Reward: 209.0 | Epsilon: 0.13 | Time: 22314.85\n",
      "Episode: 2259 | Reward: 956.0 | Epsilon: 0.13 | Time: 22541.67\n",
      "Episode: 2279 | Reward: 1856.0 | Epsilon: 0.13 | Time: 22785.94\n",
      "Episode: 2299 | Reward: 676.0 | Epsilon: 0.13 | Time: 22995.34\n",
      "Episode: 2319 | Reward: 1162.0 | Epsilon: 0.12 | Time: 23209.41\n",
      "Episode: 2339 | Reward: 525.0 | Epsilon: 0.12 | Time: 23458.06\n",
      "Episode: 2359 | Reward: 1152.0 | Epsilon: 0.12 | Time: 23693.63\n",
      "Episode: 2379 | Reward: 385.0 | Epsilon: 0.12 | Time: 23924.43\n",
      "Episode: 2399 | Reward: 749.0 | Epsilon: 0.12 | Time: 24123.12\n",
      "Episode: 2419 | Reward: 619.0 | Epsilon: 0.11 | Time: 24380.4\n",
      "Episode: 2439 | Reward: 1332.0 | Epsilon: 0.11 | Time: 24601.2\n",
      "Episode: 2459 | Reward: 842.0 | Epsilon: 0.11 | Time: 24827.29\n",
      "Episode: 2479 | Reward: 580.0 | Epsilon: 0.11 | Time: 25043.98\n",
      "Episode: 2499 | Reward: 1082.0 | Epsilon: 0.11 | Time: 25248.66\n",
      "Episode: 2519 | Reward: 1102.0 | Epsilon: 0.1 | Time: 25479.59\n",
      "Episode: 2539 | Reward: 619.0 | Epsilon: 0.1 | Time: 25703.15\n",
      "Episode: 2559 | Reward: 861.0 | Epsilon: 0.1 | Time: 25937.11\n",
      "Episode: 2579 | Reward: 1232.0 | Epsilon: 0.1 | Time: 26188.74\n",
      "Episode: 2599 | Reward: 1010.0 | Epsilon: 0.1 | Time: 26402.42\n",
      "Episode: 2619 | Reward: 592.0 | Epsilon: 0.1 | Time: 26603.07\n",
      "Episode: 2639 | Reward: 710.0 | Epsilon: 0.09 | Time: 26846.49\n",
      "Episode: 2659 | Reward: 970.0 | Epsilon: 0.09 | Time: 27087.44\n",
      "Episode: 2679 | Reward: 940.0 | Epsilon: 0.09 | Time: 27294.09\n",
      "Episode: 2699 | Reward: 293.0 | Epsilon: 0.09 | Time: 27518.47\n",
      "Episode: 2719 | Reward: 698.0 | Epsilon: 0.09 | Time: 27739.33\n",
      "Episode: 2739 | Reward: 834.0 | Epsilon: 0.09 | Time: 27946.37\n",
      "Episode: 2759 | Reward: 638.0 | Epsilon: 0.08 | Time: 28186.86\n",
      "Episode: 2779 | Reward: 1148.0 | Epsilon: 0.08 | Time: 28398.02\n",
      "Episode: 2799 | Reward: 1269.0 | Epsilon: 0.08 | Time: 28645.27\n",
      "Episode: 2819 | Reward: 855.0 | Epsilon: 0.08 | Time: 28859.56\n",
      "Episode: 2839 | Reward: 1234.0 | Epsilon: 0.08 | Time: 29089.05\n",
      "Episode: 2859 | Reward: 600.0 | Epsilon: 0.08 | Time: 29306.05\n",
      "Episode: 2879 | Reward: 785.0 | Epsilon: 0.08 | Time: 29565.55\n",
      "Episode: 2899 | Reward: 967.0 | Epsilon: 0.07 | Time: 29790.25\n",
      "Episode: 2919 | Reward: 262.0 | Epsilon: 0.07 | Time: 30030.19\n",
      "Episode: 2939 | Reward: 1087.0 | Epsilon: 0.07 | Time: 30267.94\n",
      "Episode: 2959 | Reward: 558.0 | Epsilon: 0.07 | Time: 30497.63\n",
      "Episode: 2979 | Reward: 1091.0 | Epsilon: 0.07 | Time: 30729.06\n",
      "Episode: 2999 | Reward: 453.0 | Epsilon: 0.07 | Time: 30976.36\n",
      "Episode: 3019 | Reward: 848.0 | Epsilon: 0.07 | Time: 31217.9\n",
      "Episode: 3039 | Reward: 743.0 | Epsilon: 0.07 | Time: 31467.47\n",
      "Episode: 3059 | Reward: 592.0 | Epsilon: 0.06 | Time: 31681.27\n",
      "Episode: 3079 | Reward: 1163.0 | Epsilon: 0.06 | Time: 31905.21\n",
      "Episode: 3099 | Reward: 1236.0 | Epsilon: 0.06 | Time: 32134.36\n",
      "Episode: 3119 | Reward: 498.0 | Epsilon: 0.06 | Time: 32357.07\n",
      "Episode: 3139 | Reward: 535.0 | Epsilon: 0.06 | Time: 32603.05\n",
      "Episode: 3159 | Reward: 1712.0 | Epsilon: 0.06 | Time: 32843.04\n",
      "Episode: 3179 | Reward: 1061.0 | Epsilon: 0.06 | Time: 33121.94\n",
      "Episode: 3199 | Reward: 1222.0 | Epsilon: 0.06 | Time: 33356.89\n",
      "Episode: 3219 | Reward: 1781.0 | Epsilon: 0.06 | Time: 33581.59\n",
      "Episode: 3239 | Reward: 1018.0 | Epsilon: 0.06 | Time: 33801.94\n",
      "Episode: 3259 | Reward: 677.0 | Epsilon: 0.05 | Time: 34015.44\n",
      "Episode: 3279 | Reward: 1365.0 | Epsilon: 0.05 | Time: 48220.09\n",
      "Episode: 3299 | Reward: 572.0 | Epsilon: 0.05 | Time: 48522.79\n",
      "Episode: 3319 | Reward: 666.0 | Epsilon: 0.05 | Time: 48803.02\n",
      "Episode: 3339 | Reward: 827.0 | Epsilon: 0.05 | Time: 49052.16\n",
      "Episode: 3359 | Reward: 1158.0 | Epsilon: 0.05 | Time: 49321.43\n",
      "Episode: 3379 | Reward: 560.0 | Epsilon: 0.05 | Time: 49572.73\n",
      "Episode: 3399 | Reward: 1055.0 | Epsilon: 0.05 | Time: 49790.22\n",
      "Episode: 3419 | Reward: 1353.0 | Epsilon: 0.05 | Time: 50021.0\n",
      "Episode: 3439 | Reward: -111.0 | Epsilon: 0.05 | Time: 50242.01\n",
      "Episode: 3459 | Reward: 570.0 | Epsilon: 0.05 | Time: 50449.83\n",
      "Episode: 3479 | Reward: 986.0 | Epsilon: 0.04 | Time: 50680.44\n",
      "Episode: 3499 | Reward: 499.0 | Epsilon: 0.04 | Time: 50924.78\n",
      "Episode: 3519 | Reward: 355.0 | Epsilon: 0.04 | Time: 51144.87\n",
      "Episode: 3539 | Reward: 902.0 | Epsilon: 0.04 | Time: 51382.45\n",
      "Episode: 3559 | Reward: 1388.0 | Epsilon: 0.04 | Time: 51620.46\n",
      "Episode: 3579 | Reward: 559.0 | Epsilon: 0.04 | Time: 51836.37\n",
      "Episode: 3599 | Reward: 856.0 | Epsilon: 0.04 | Time: 52067.06\n",
      "Episode: 3619 | Reward: 446.0 | Epsilon: 0.04 | Time: 52288.45\n",
      "Episode: 3639 | Reward: 348.0 | Epsilon: 0.04 | Time: 52495.66\n",
      "Episode: 3659 | Reward: 473.0 | Epsilon: 0.04 | Time: 52691.95\n",
      "Episode: 3679 | Reward: 988.0 | Epsilon: 0.04 | Time: 52922.83\n",
      "Episode: 3699 | Reward: 455.0 | Epsilon: 0.04 | Time: 53109.86\n",
      "Episode: 3719 | Reward: 846.0 | Epsilon: 0.04 | Time: 53361.18\n",
      "Episode: 3739 | Reward: 431.0 | Epsilon: 0.04 | Time: 53590.08\n",
      "Episode: 3759 | Reward: 1034.0 | Epsilon: 0.03 | Time: 53841.54\n",
      "Episode: 3779 | Reward: 648.0 | Epsilon: 0.03 | Time: 54067.5\n",
      "Episode: 3799 | Reward: 658.0 | Epsilon: 0.03 | Time: 54297.74\n",
      "Episode: 3819 | Reward: 1168.0 | Epsilon: 0.03 | Time: 54516.98\n",
      "Episode: 3839 | Reward: 1179.0 | Epsilon: 0.03 | Time: 54765.25\n",
      "Episode: 3859 | Reward: 1908.0 | Epsilon: 0.03 | Time: 54999.11\n",
      "Episode: 3879 | Reward: 918.0 | Epsilon: 0.03 | Time: 55226.7\n",
      "Episode: 3899 | Reward: 394.0 | Epsilon: 0.03 | Time: 55437.8\n",
      "Episode: 3919 | Reward: 117.0 | Epsilon: 0.03 | Time: 55666.22\n",
      "Episode: 3939 | Reward: 704.0 | Epsilon: 0.03 | Time: 55928.38\n",
      "Episode: 3959 | Reward: 852.0 | Epsilon: 0.03 | Time: 56151.52\n",
      "Episode: 3979 | Reward: 242.0 | Epsilon: 0.03 | Time: 56370.18\n",
      "Episode: 3999 | Reward: 235.0 | Epsilon: 0.03 | Time: 56593.69\n",
      "Episode: 4019 | Reward: 409.0 | Epsilon: 0.03 | Time: 56820.23\n",
      "Episode: 4039 | Reward: 1131.0 | Epsilon: 0.03 | Time: 57060.35\n",
      "Episode: 4059 | Reward: 317.0 | Epsilon: 0.03 | Time: 57282.97\n",
      "Episode: 4079 | Reward: 699.0 | Epsilon: 0.03 | Time: 57549.5\n",
      "Episode: 4099 | Reward: 1368.0 | Epsilon: 0.03 | Time: 57778.8\n",
      "Episode: 4119 | Reward: 203.0 | Epsilon: 0.03 | Time: 57984.5\n",
      "Episode: 4139 | Reward: 694.0 | Epsilon: 0.03 | Time: 58228.14\n",
      "Episode: 4159 | Reward: 623.0 | Epsilon: 0.02 | Time: 58473.59\n",
      "Episode: 4179 | Reward: 1178.0 | Epsilon: 0.02 | Time: 58689.93\n",
      "Episode: 4199 | Reward: 1539.0 | Epsilon: 0.02 | Time: 58929.64\n",
      "Episode: 4219 | Reward: 1093.0 | Epsilon: 0.02 | Time: 59140.47\n",
      "Episode: 4239 | Reward: 861.0 | Epsilon: 0.02 | Time: 59356.96\n",
      "Episode: 4259 | Reward: 534.0 | Epsilon: 0.02 | Time: 59597.86\n",
      "Episode: 4279 | Reward: 576.0 | Epsilon: 0.02 | Time: 59820.44\n",
      "Episode: 4299 | Reward: 1521.0 | Epsilon: 0.02 | Time: 60063.01\n",
      "Episode: 4319 | Reward: 1517.0 | Epsilon: 0.02 | Time: 60277.94\n",
      "Episode: 4339 | Reward: 1701.0 | Epsilon: 0.02 | Time: 60519.67\n",
      "Episode: 4359 | Reward: 250.0 | Epsilon: 0.02 | Time: 60750.64\n",
      "Episode: 4379 | Reward: 848.0 | Epsilon: 0.02 | Time: 60997.6\n",
      "Episode: 4399 | Reward: 386.0 | Epsilon: 0.02 | Time: 61235.23\n",
      "Episode: 4419 | Reward: 925.0 | Epsilon: 0.02 | Time: 61483.99\n",
      "Episode: 4439 | Reward: 604.0 | Epsilon: 0.02 | Time: 61754.21\n",
      "Episode: 4459 | Reward: 729.0 | Epsilon: 0.02 | Time: 61994.54\n",
      "Episode: 4479 | Reward: 585.0 | Epsilon: 0.02 | Time: 62238.93\n",
      "Episode: 4499 | Reward: 1744.0 | Epsilon: 0.02 | Time: 62482.81\n",
      "Episode: 4519 | Reward: 1294.0 | Epsilon: 0.02 | Time: 62729.06\n",
      "Episode: 4539 | Reward: 1107.0 | Epsilon: 0.02 | Time: 62967.8\n",
      "Episode: 4559 | Reward: 343.0 | Epsilon: 0.02 | Time: 63205.64\n",
      "Episode: 4579 | Reward: 688.0 | Epsilon: 0.02 | Time: 63450.5\n",
      "Episode: 4599 | Reward: 986.0 | Epsilon: 0.02 | Time: 63692.87\n",
      "Episode: 4619 | Reward: 910.0 | Epsilon: 0.02 | Time: 63916.33\n",
      "Episode: 4639 | Reward: 445.0 | Epsilon: 0.02 | Time: 64166.57\n",
      "Episode: 4659 | Reward: 1086.0 | Epsilon: 0.02 | Time: 64394.27\n",
      "Episode: 4679 | Reward: 448.0 | Epsilon: 0.02 | Time: 64605.03\n",
      "Episode: 4699 | Reward: 189.0 | Epsilon: 0.02 | Time: 64829.78\n",
      "Episode: 4719 | Reward: 784.0 | Epsilon: 0.02 | Time: 65054.2\n",
      "Episode: 4739 | Reward: 763.0 | Epsilon: 0.02 | Time: 65296.85\n",
      "Episode: 4759 | Reward: 1547.0 | Epsilon: 0.01 | Time: 65548.83\n",
      "Episode: 4779 | Reward: 595.0 | Epsilon: 0.01 | Time: 65787.72\n",
      "Episode: 4799 | Reward: 203.0 | Epsilon: 0.01 | Time: 66031.52\n",
      "Episode: 4819 | Reward: 1107.0 | Epsilon: 0.01 | Time: 66267.04\n",
      "Episode: 4839 | Reward: 1229.0 | Epsilon: 0.01 | Time: 66504.75\n",
      "Episode: 4859 | Reward: 878.0 | Epsilon: 0.01 | Time: 66753.32\n",
      "Episode: 4879 | Reward: 708.0 | Epsilon: 0.01 | Time: 66995.04\n",
      "Episode: 4899 | Reward: 1296.0 | Epsilon: 0.01 | Time: 67222.83\n",
      "Episode: 4919 | Reward: 303.0 | Epsilon: 0.01 | Time: 67468.62\n",
      "Episode: 4939 | Reward: 367.0 | Epsilon: 0.01 | Time: 67713.99\n",
      "Episode: 4959 | Reward: 884.0 | Epsilon: 0.01 | Time: 67956.16\n",
      "Episode: 4979 | Reward: 794.0 | Epsilon: 0.01 | Time: 68203.98\n",
      "Episode: 4999 | Reward: 1088.0 | Epsilon: 0.01 | Time: 68454.29\n",
      "Episode: 5019 | Reward: 761.0 | Epsilon: 0.01 | Time: 68690.72\n",
      "Episode: 5039 | Reward: 1049.0 | Epsilon: 0.01 | Time: 68933.1\n",
      "Episode: 5059 | Reward: 473.0 | Epsilon: 0.01 | Time: 69173.0\n",
      "Episode: 5079 | Reward: 1527.0 | Epsilon: 0.01 | Time: 69406.67\n",
      "Episode: 5099 | Reward: 1008.0 | Epsilon: 0.01 | Time: 69642.17\n",
      "Episode: 5119 | Reward: 1071.0 | Epsilon: 0.01 | Time: 69871.65\n",
      "Episode: 5139 | Reward: 5.0 | Epsilon: 0.01 | Time: 70101.41\n",
      "Episode: 5159 | Reward: 476.0 | Epsilon: 0.01 | Time: 70337.66\n",
      "Episode: 5179 | Reward: 171.0 | Epsilon: 0.01 | Time: 70577.12\n",
      "Episode: 5199 | Reward: 70.0 | Epsilon: 0.01 | Time: 70832.73\n",
      "Episode: 5219 | Reward: 1463.0 | Epsilon: 0.01 | Time: 71082.92\n",
      "Episode: 5239 | Reward: 363.0 | Epsilon: 0.01 | Time: 159169.97\n",
      "Episode: 5259 | Reward: 662.0 | Epsilon: 0.01 | Time: 159386.55\n",
      "Episode: 5279 | Reward: 1081.0 | Epsilon: 0.01 | Time: 159644.82\n",
      "Episode: 5299 | Reward: 830.0 | Epsilon: 0.01 | Time: 159909.31\n",
      "Episode: 5319 | Reward: 336.0 | Epsilon: 0.01 | Time: 160180.29\n",
      "Episode: 5339 | Reward: 914.0 | Epsilon: 0.01 | Time: 160445.66\n",
      "Episode: 5359 | Reward: 529.0 | Epsilon: 0.01 | Time: 160709.65\n",
      "Episode: 5379 | Reward: 749.0 | Epsilon: 0.01 | Time: 160948.31\n",
      "Episode: 5399 | Reward: 178.0 | Epsilon: 0.01 | Time: 161158.95\n",
      "Episode: 5419 | Reward: 981.0 | Epsilon: 0.01 | Time: 161385.87\n",
      "Episode: 5439 | Reward: 878.0 | Epsilon: 0.01 | Time: 161588.28\n",
      "Episode: 5459 | Reward: 2215.0 | Epsilon: 0.01 | Time: 161840.62\n",
      "Episode: 5479 | Reward: 2063.0 | Epsilon: 0.01 | Time: 162103.49\n",
      "Episode: 5499 | Reward: 752.0 | Epsilon: 0.01 | Time: 162341.07\n",
      "Episode: 5519 | Reward: 1040.0 | Epsilon: 0.01 | Time: 162628.13\n",
      "Episode: 5539 | Reward: 536.0 | Epsilon: 0.01 | Time: 162881.95\n",
      "Episode: 5559 | Reward: 828.0 | Epsilon: 0.01 | Time: 163132.81\n",
      "Episode: 5579 | Reward: 354.0 | Epsilon: 0.01 | Time: 163374.51\n",
      "Episode: 5599 | Reward: 576.0 | Epsilon: 0.01 | Time: 163591.27\n",
      "Episode: 5619 | Reward: 119.0 | Epsilon: 0.01 | Time: 163829.48\n",
      "Episode: 5639 | Reward: 28.0 | Epsilon: 0.01 | Time: 164070.73\n",
      "Episode: 5659 | Reward: 104.0 | Epsilon: 0.01 | Time: 164310.98\n",
      "Episode: 5679 | Reward: 1192.0 | Epsilon: 0.01 | Time: 164560.42\n",
      "Episode: 5699 | Reward: 1316.0 | Epsilon: 0.01 | Time: 164819.82\n",
      "Episode: 5719 | Reward: 362.0 | Epsilon: 0.01 | Time: 165043.35\n",
      "Episode: 5739 | Reward: 928.0 | Epsilon: 0.01 | Time: 165282.17\n",
      "Episode: 5759 | Reward: 1418.0 | Epsilon: 0.01 | Time: 165548.11\n",
      "Episode: 5779 | Reward: 710.0 | Epsilon: 0.01 | Time: 165772.94\n",
      "Episode: 5799 | Reward: 1460.0 | Epsilon: 0.01 | Time: 166021.15\n",
      "Episode: 5819 | Reward: 797.0 | Epsilon: 0.01 | Time: 166275.72\n",
      "Episode: 5839 | Reward: 1340.0 | Epsilon: 0.01 | Time: 166539.37\n",
      "Episode: 5859 | Reward: 1937.0 | Epsilon: 0.01 | Time: 166793.35\n",
      "Episode: 5879 | Reward: 1375.0 | Epsilon: 0.01 | Time: 167075.74\n",
      "Episode: 5899 | Reward: 1864.0 | Epsilon: 0.01 | Time: 167343.18\n",
      "Episode: 5919 | Reward: 949.0 | Epsilon: 0.01 | Time: 167591.17\n",
      "Episode: 5939 | Reward: 351.0 | Epsilon: 0.01 | Time: 167848.45\n",
      "Episode: 5959 | Reward: 1032.0 | Epsilon: 0.01 | Time: 168100.44\n",
      "Episode: 5979 | Reward: 829.0 | Epsilon: 0.01 | Time: 168345.62\n",
      "Episode: 5999 | Reward: 55.0 | Epsilon: 0.01 | Time: 168584.58\n",
      "Episode: 6019 | Reward: 505.0 | Epsilon: 0.01 | Time: 168840.35\n",
      "Episode: 6039 | Reward: 542.0 | Epsilon: 0.01 | Time: 169070.94\n",
      "Episode: 6059 | Reward: 11.0 | Epsilon: 0.01 | Time: 169290.95\n",
      "Episode: 6079 | Reward: 405.0 | Epsilon: 0.01 | Time: 186631.62\n",
      "Episode: 6099 | Reward: 1760.0 | Epsilon: 0.01 | Time: 186896.63\n",
      "Episode: 6119 | Reward: 657.0 | Epsilon: 0.01 | Time: 187121.19\n",
      "Episode: 6139 | Reward: 908.0 | Epsilon: 0.0 | Time: 187386.8\n",
      "Episode: 6159 | Reward: 1514.0 | Epsilon: 0.0 | Time: 187622.29\n",
      "Episode: 6179 | Reward: 515.0 | Epsilon: 0.0 | Time: 187850.31\n",
      "Episode: 6199 | Reward: 1134.0 | Epsilon: 0.0 | Time: 188101.47\n",
      "Episode: 6219 | Reward: 809.0 | Epsilon: 0.0 | Time: 188339.06\n",
      "Episode: 6239 | Reward: 247.0 | Epsilon: 0.0 | Time: 188595.95\n",
      "Episode: 6259 | Reward: 2151.0 | Epsilon: 0.0 | Time: 188851.37\n",
      "Episode: 6279 | Reward: 1356.0 | Epsilon: 0.0 | Time: 189079.7\n",
      "Episode: 6299 | Reward: 1139.0 | Epsilon: 0.0 | Time: 189308.29\n",
      "Episode: 6319 | Reward: 1548.0 | Epsilon: 0.0 | Time: 189548.57\n",
      "Episode: 6339 | Reward: 94.0 | Epsilon: 0.0 | Time: 189785.0\n",
      "Episode: 6359 | Reward: 943.0 | Epsilon: 0.0 | Time: 190034.86\n",
      "Episode: 6379 | Reward: 445.0 | Epsilon: 0.0 | Time: 190258.56\n",
      "Episode: 6399 | Reward: 896.0 | Epsilon: 0.0 | Time: 190473.92\n",
      "Episode: 6419 | Reward: 554.0 | Epsilon: 0.0 | Time: 190732.89\n",
      "Episode: 6439 | Reward: 327.0 | Epsilon: 0.0 | Time: 190998.32\n",
      "Episode: 6459 | Reward: 1331.0 | Epsilon: 0.0 | Time: 191256.14\n",
      "Episode: 6479 | Reward: 165.0 | Epsilon: 0.0 | Time: 191478.86\n",
      "Episode: 6499 | Reward: 1299.0 | Epsilon: 0.0 | Time: 191728.92\n",
      "Episode: 6519 | Reward: 497.0 | Epsilon: 0.0 | Time: 191990.69\n",
      "Episode: 6539 | Reward: 437.0 | Epsilon: 0.0 | Time: 192230.29\n",
      "Episode: 6559 | Reward: 94.0 | Epsilon: 0.0 | Time: 192457.16\n",
      "Episode: 6579 | Reward: 378.0 | Epsilon: 0.0 | Time: 192704.69\n",
      "Episode: 6599 | Reward: -14.0 | Epsilon: 0.0 | Time: 192928.18\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for episode in range(Episodes):\n",
    "    # Write code here    \n",
    "    \n",
    "    # Call all the initialised variables of the environment\n",
    "    action_space = copy.deepcopy(env.action_space)\n",
    "    state_space = copy.deepcopy(env.state_space)\n",
    "    curr_state = env.state_init\n",
    "    action_size = len(action_space)\n",
    "    state_size = len(env.state_encod_arch1(env.state_init))\n",
    "    terminal_state = False\n",
    "    episode_time = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    #Call the DQN agent\n",
    "    agent = DQNAgent(state_size,action_size)\n",
    "    \n",
    "    while not terminal_state:\n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(curr_state)\n",
    "#         print(action)\n",
    "        \n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = env.reward_func(curr_state, action, Time_matrix)\n",
    "        next_state,time_lapsed = env.next_state_func(curr_state, action, Time_matrix)\n",
    "        \n",
    "        episode_time += time_lapsed\n",
    "        \n",
    "        if episode_time > 720:\n",
    "            terminal_state = True\n",
    "        else:\n",
    "            # 3. Append the experience <s,a,s',r,t> to the memory \n",
    "            agent.append_sample(curr_state, action, reward, next_state, terminal_state)\n",
    "\n",
    "            # 4. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            \n",
    "            # 5. Keep a track of rewards, Q-values, loss\n",
    "            total_reward += reward\n",
    "            next_state = curr_state\n",
    "\n",
    "    #TRACKING Q-VALUES      \n",
    "    if ((episode+1) % 20) == 0:\n",
    "        save_total_rewards(total_reward,episode)\n",
    "        for s in States_track.keys():\n",
    "            state_encoded = np.array(env.state_encod_arch1(s)).reshape(1, 36)\n",
    "            qval = agent.model.predict(state_encoded)\n",
    "            save_tracking_states(s, qval)\n",
    "        save_obj(Rewards_track,'Reward_tracked')\n",
    "        save_obj(States_track,'States_tracked')\n",
    "        #Save Model Weights\n",
    "        agent.save('model_weights')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    if ((episode+1) % 20) == 0:   \n",
    "        print(\"Episode: \"+str(episode)+\" | Reward: \"+str(total_reward)+\" | Epsilon: \"+str(round(agent.epsilon,2)) \n",
    "             +\" | Time: \"+str(round(elapsed_time,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING:tensorflow:5 out of the last 19 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000019E65037948> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,500)\n",
    "epsilon = []\n",
    "for i in range(0,500):\n",
    "    epsilon.append(0.001 + (1 - 0.001) * np.exp(-0.009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "with open('Reward_tracked.h5', 'rb') as handle:\n",
    "    Rewards = pickle.load(handle)\n",
    "with open('States_tracked.h5', 'rb') as handle:\n",
    "    States_track = pickle.load(handle)    \n",
    "# with open('model_weights.h5', 'rb') as handle:\n",
    "#     model_weights = pickle.load(handle)   \n",
    "    \n",
    "print(len(Rewards))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Rewards['Episode'], Rewards['Rewards'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
