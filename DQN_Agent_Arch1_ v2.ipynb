{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File Paths\n",
    "timeMatrix_path = \"inputs/TM.npy\"\n",
    "\n",
    "# Importing the environment\n",
    "from references.Env import CabDriver\n",
    "\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Test Libs\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the environment\n",
    "env = CabDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(timeMatrix_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcxZ29vdGggS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Initialise Q_dictionary as 'Q_dict' and States_tracked as 'States_track' (for convergence)\n",
    "States_track = collections.defaultdict(dict)\n",
    "Rewards_track = collections.defaultdict(dict)\n",
    "\n",
    "print(len(States_track))\n",
    "print(len(Rewards_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "def initialise_tracking_states():\n",
    "#     self.track_state = np.array(env.state_encod_arch1([0,7,4])).reshape(1, 36)\n",
    "    sample_states = [(0, 7, 4)]\n",
    "    for state in sample_states:\n",
    "        for action in env.action_space:\n",
    "            States_track[state][action] = []\n",
    "    Rewards_track['Rewards'] = []\n",
    "    Rewards_track['Episode'] = []\n",
    "\n",
    "# Defining a function to track the states initialized\n",
    "def save_tracking_states(state, qval):\n",
    "    if state in States_track.keys():\n",
    "        for i,action in enumerate(States_track[state].keys()):\n",
    "            States_track[state][action].append(qval[0][i])\n",
    "            \n",
    "# Defining a function to track the states initialized\n",
    "def save_total_rewards(total_reward, episode_num):\n",
    "    Rewards_track['Rewards'].append(total_reward)\n",
    "    Rewards_track['Episode'].append(episode_num)\n",
    "\n",
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.h5', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size \n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.5\n",
    "        self.learning_rate = 0.01      \n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = 0.005\n",
    "        self.epsilon_min = 0.00001\n",
    "        self.batch_size = 32      \n",
    "        self.epsilon = 1\n",
    "                \n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):  \n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32,activation='relu',input_dim=self.state_size, kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32,activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # Write your code here:\n",
    "        self.epsilon = self.epsilon_min + (self.epsilon_max - self.epsilon_min) * np.exp(-self.epsilon_decay *episode)\n",
    "        self.z = np.random.random()  \n",
    "        possible_actions_index, possible_actions = env.requests(state)\n",
    "\n",
    "        # get action from model using epsilon-greedy policy\n",
    "        if self.z > self.epsilon:\n",
    "            #Greedy action\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "            q_value = self.model.predict(state)\n",
    "#             print(\"q_value\")\n",
    "#             print(q_value)\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "#             print(q_vals_possible)\n",
    "            actionIndex = possible_actions_index[np.argmax(q_vals_possible)]\n",
    "            action = env.action_space[actionIndex]\n",
    "            \n",
    "        # Decay in Îµ after we generate each sample from the environment       \n",
    "        else:\n",
    "            #Random\n",
    "            action = random.choice(possible_actions)\n",
    "        return action\n",
    "        \n",
    "    def append_sample(self, state, action, reward, next_state,terminal_state):\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state,terminal_state))\n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size: \n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            \n",
    "            actions, rewards, terminal_states = [], [], []\n",
    "            \n",
    "            #TRAIN THE MODEL\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, terminal_state = mini_batch[i]\n",
    "#                 print(actions)\n",
    "                actions.append(env.action_space.index(action))\n",
    "                rewards.append(reward)\n",
    "                terminal_states.append(terminal_state)\n",
    "#                 print(\"old_next_state\")\n",
    "#                 print(next_state)\n",
    "#                 print(\"new_next_state\")\n",
    "                \n",
    "                # Write your code from here\n",
    "                #1. Update your 'update_output' and 'update_input' batch\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "            \n",
    "            # 2. Predict the target from earlier model\n",
    "            target = self.model.predict(update_input)\n",
    "            target_nextqval = self.model.predict(update_output)\n",
    "            \n",
    "            # 3. Get the target for the Q-network\n",
    "            for i in range(self.batch_size):\n",
    "                if terminal_state:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor* np.max(target_nextqval[i])\n",
    "            \n",
    "            # 4. Fit your model and track the loss values\n",
    "#             print(\"update_input\")\n",
    "#             print(update_input[0])\n",
    "#             print(\"target\")\n",
    "            self.model.fit(update_input, target, batch_size = self.batch_size, epochs=1, verbose=0 )\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 19 | Reward: 534.0 | Epsilon: 0.91 | Time: 269.67\n",
      "Episode: 39 | Reward: 668.0 | Epsilon: 0.82 | Time: 544.75\n",
      "Episode: 59 | Reward: 639.0 | Epsilon: 0.74 | Time: 799.36\n",
      "Episode: 79 | Reward: 702.0 | Epsilon: 0.67 | Time: 1062.01\n",
      "Episode: 99 | Reward: 454.0 | Epsilon: 0.61 | Time: 1294.67\n",
      "Episode: 119 | Reward: 675.0 | Epsilon: 0.55 | Time: 1529.73\n",
      "Episode: 139 | Reward: 1238.0 | Epsilon: 0.5 | Time: 1817.52\n",
      "Episode: 159 | Reward: 1284.0 | Epsilon: 0.45 | Time: 2090.01\n",
      "Episode: 179 | Reward: 1344.0 | Epsilon: 0.41 | Time: 2338.19\n",
      "Episode: 199 | Reward: 923.0 | Epsilon: 0.37 | Time: 2579.99\n",
      "Episode: 219 | Reward: 1395.0 | Epsilon: 0.33 | Time: 2809.69\n",
      "Episode: 239 | Reward: 809.0 | Epsilon: 0.3 | Time: 3085.72\n",
      "Episode: 259 | Reward: 1049.0 | Epsilon: 0.27 | Time: 3354.47\n",
      "Episode: 279 | Reward: 1020.0 | Epsilon: 0.25 | Time: 3626.87\n",
      "Episode: 299 | Reward: 824.0 | Epsilon: 0.22 | Time: 3922.69\n",
      "Episode: 319 | Reward: 1132.0 | Epsilon: 0.2 | Time: 4205.63\n",
      "Episode: 339 | Reward: 1413.0 | Epsilon: 0.18 | Time: 4491.92\n",
      "Episode: 359 | Reward: 882.0 | Epsilon: 0.17 | Time: 4731.17\n",
      "Episode: 379 | Reward: 1088.0 | Epsilon: 0.15 | Time: 4997.5\n",
      "Episode: 399 | Reward: 1207.0 | Epsilon: 0.14 | Time: 5280.87\n",
      "Episode: 419 | Reward: 438.0 | Epsilon: 0.12 | Time: 5569.37\n",
      "Episode: 439 | Reward: 699.0 | Epsilon: 0.11 | Time: 5835.04\n",
      "Episode: 459 | Reward: 1145.0 | Epsilon: 0.1 | Time: 6111.89\n",
      "Episode: 479 | Reward: 1463.0 | Epsilon: 0.09 | Time: 6361.94\n",
      "Episode: 499 | Reward: 1415.0 | Epsilon: 0.08 | Time: 6602.4\n",
      "Episode: 519 | Reward: 562.0 | Epsilon: 0.07 | Time: 6852.58\n",
      "Episode: 539 | Reward: 1498.0 | Epsilon: 0.07 | Time: 7107.91\n",
      "Episode: 559 | Reward: 1385.0 | Epsilon: 0.06 | Time: 7368.11\n",
      "Episode: 579 | Reward: 1721.0 | Epsilon: 0.06 | Time: 7611.01\n",
      "Episode: 599 | Reward: 1245.0 | Epsilon: 0.05 | Time: 7881.65\n",
      "Episode: 619 | Reward: 1420.0 | Epsilon: 0.05 | Time: 8148.88\n",
      "Episode: 639 | Reward: 1727.0 | Epsilon: 0.04 | Time: 8379.77\n",
      "Episode: 659 | Reward: 1182.0 | Epsilon: 0.04 | Time: 8622.62\n",
      "Episode: 679 | Reward: 1306.0 | Epsilon: 0.03 | Time: 8894.65\n",
      "Episode: 699 | Reward: 1275.0 | Epsilon: 0.03 | Time: 9147.1\n",
      "Episode: 719 | Reward: 1116.0 | Epsilon: 0.03 | Time: 9394.29\n",
      "Episode: 739 | Reward: 971.0 | Epsilon: 0.02 | Time: 9659.05\n",
      "Episode: 759 | Reward: 810.0 | Epsilon: 0.02 | Time: 9931.67\n",
      "Episode: 779 | Reward: 844.0 | Epsilon: 0.02 | Time: 10213.03\n",
      "Episode: 799 | Reward: 1157.0 | Epsilon: 0.02 | Time: 10470.08\n",
      "Episode: 819 | Reward: 197.0 | Epsilon: 0.02 | Time: 10707.73\n",
      "Episode: 839 | Reward: 734.0 | Epsilon: 0.02 | Time: 10986.78\n",
      "Episode: 859 | Reward: 1559.0 | Epsilon: 0.01 | Time: 11242.82\n",
      "Episode: 879 | Reward: 1676.0 | Epsilon: 0.01 | Time: 11521.45\n",
      "Episode: 899 | Reward: 1158.0 | Epsilon: 0.01 | Time: 11769.38\n",
      "Episode: 919 | Reward: 857.0 | Epsilon: 0.01 | Time: 12018.39\n",
      "Episode: 939 | Reward: 1765.0 | Epsilon: 0.01 | Time: 12259.31\n",
      "Episode: 959 | Reward: 1590.0 | Epsilon: 0.01 | Time: 12517.99\n",
      "Episode: 979 | Reward: 856.0 | Epsilon: 0.01 | Time: 12769.22\n",
      "Episode: 999 | Reward: 78.0 | Epsilon: 0.01 | Time: 13021.48\n",
      "Episode: 1019 | Reward: 1097.0 | Epsilon: 0.01 | Time: 13269.74\n",
      "Episode: 1039 | Reward: 1453.0 | Epsilon: 0.01 | Time: 13551.96\n",
      "Episode: 1059 | Reward: 1263.0 | Epsilon: 0.01 | Time: 13812.14\n",
      "Episode: 1079 | Reward: 1795.0 | Epsilon: 0.0 | Time: 14057.05\n",
      "Episode: 1099 | Reward: 1524.0 | Epsilon: 0.0 | Time: 14317.06\n",
      "Episode: 1119 | Reward: 1048.0 | Epsilon: 0.0 | Time: 14583.7\n",
      "Episode: 1139 | Reward: 1005.0 | Epsilon: 0.0 | Time: 14851.19\n",
      "Episode: 1159 | Reward: 1746.0 | Epsilon: 0.0 | Time: 15102.31\n",
      "Episode: 1179 | Reward: 1146.0 | Epsilon: 0.0 | Time: 15360.23\n",
      "Episode: 1199 | Reward: 1709.0 | Epsilon: 0.0 | Time: 15614.01\n",
      "Episode: 1219 | Reward: 1118.0 | Epsilon: 0.0 | Time: 15867.04\n",
      "Episode: 1239 | Reward: 1650.0 | Epsilon: 0.0 | Time: 16126.58\n",
      "Episode: 1259 | Reward: 568.0 | Epsilon: 0.0 | Time: 16386.17\n",
      "Episode: 1279 | Reward: 811.0 | Epsilon: 0.0 | Time: 16657.68\n",
      "Episode: 1299 | Reward: 929.0 | Epsilon: 0.0 | Time: 16916.6\n",
      "Episode: 1319 | Reward: 461.0 | Epsilon: 0.0 | Time: 17168.64\n",
      "Episode: 1339 | Reward: 1694.0 | Epsilon: 0.0 | Time: 17419.97\n",
      "Episode: 1359 | Reward: 1667.0 | Epsilon: 0.0 | Time: 17685.59\n",
      "Episode: 1379 | Reward: 1371.0 | Epsilon: 0.0 | Time: 17949.2\n",
      "Episode: 1399 | Reward: 1003.0 | Epsilon: 0.0 | Time: 18207.06\n",
      "Episode: 1419 | Reward: 1085.0 | Epsilon: 0.0 | Time: 18456.94\n",
      "Episode: 1439 | Reward: 872.0 | Epsilon: 0.0 | Time: 18716.94\n",
      "Episode: 1459 | Reward: 1045.0 | Epsilon: 0.0 | Time: 18977.82\n",
      "Episode: 1479 | Reward: 615.0 | Epsilon: 0.0 | Time: 19234.33\n",
      "Episode: 1499 | Reward: 599.0 | Epsilon: 0.0 | Time: 19502.48\n",
      "Episode: 1519 | Reward: 1000.0 | Epsilon: 0.0 | Time: 19763.83\n",
      "Episode: 1539 | Reward: 1143.0 | Epsilon: 0.0 | Time: 20013.02\n",
      "Episode: 1559 | Reward: 16.0 | Epsilon: 0.0 | Time: 20254.16\n",
      "Episode: 1579 | Reward: 1207.0 | Epsilon: 0.0 | Time: 20523.39\n",
      "Episode: 1599 | Reward: 1293.0 | Epsilon: 0.0 | Time: 20805.21\n",
      "Episode: 1619 | Reward: 1073.0 | Epsilon: 0.0 | Time: 21067.05\n",
      "Episode: 1639 | Reward: 1265.0 | Epsilon: 0.0 | Time: 21344.27\n",
      "Episode: 1659 | Reward: 1055.0 | Epsilon: 0.0 | Time: 21626.08\n",
      "Episode: 1679 | Reward: 676.0 | Epsilon: 0.0 | Time: 21870.31\n",
      "Episode: 1699 | Reward: 829.0 | Epsilon: 0.0 | Time: 22108.16\n",
      "Episode: 1719 | Reward: 850.0 | Epsilon: 0.0 | Time: 22351.06\n",
      "Episode: 1739 | Reward: 1655.0 | Epsilon: 0.0 | Time: 22601.92\n",
      "Episode: 1759 | Reward: 1251.0 | Epsilon: 0.0 | Time: 22853.94\n",
      "Episode: 1779 | Reward: 1430.0 | Epsilon: 0.0 | Time: 23096.46\n",
      "Episode: 1799 | Reward: 1476.0 | Epsilon: 0.0 | Time: 23358.2\n",
      "Episode: 1819 | Reward: 1321.0 | Epsilon: 0.0 | Time: 23619.16\n",
      "Episode: 1839 | Reward: 303.0 | Epsilon: 0.0 | Time: 23879.45\n",
      "Episode: 1859 | Reward: 1509.0 | Epsilon: 0.0 | Time: 24142.55\n",
      "Episode: 1879 | Reward: 1294.0 | Epsilon: 0.0 | Time: 24387.3\n",
      "Episode: 1899 | Reward: 1428.0 | Epsilon: 0.0 | Time: 24637.05\n",
      "Episode: 1919 | Reward: 1616.0 | Epsilon: 0.0 | Time: 24888.83\n",
      "Episode: 1939 | Reward: 1707.0 | Epsilon: 0.0 | Time: 25130.9\n",
      "Episode: 1959 | Reward: 1490.0 | Epsilon: 0.0 | Time: 25399.82\n",
      "Episode: 1979 | Reward: 1638.0 | Epsilon: 0.0 | Time: 25675.43\n",
      "Episode: 1999 | Reward: 1516.0 | Epsilon: 0.0 | Time: 25932.78\n",
      "Episode: 2019 | Reward: 1583.0 | Epsilon: 0.0 | Time: 26179.59\n",
      "Episode: 2039 | Reward: 1357.0 | Epsilon: 0.0 | Time: 26440.38\n",
      "Episode: 2059 | Reward: 958.0 | Epsilon: 0.0 | Time: 26714.05\n",
      "Episode: 2079 | Reward: 1340.0 | Epsilon: 0.0 | Time: 26973.87\n",
      "Episode: 2099 | Reward: 984.0 | Epsilon: 0.0 | Time: 27215.72\n",
      "Episode: 2119 | Reward: 578.0 | Epsilon: 0.0 | Time: 27475.74\n",
      "Episode: 2139 | Reward: 1380.0 | Epsilon: 0.0 | Time: 27723.1\n",
      "Episode: 2159 | Reward: 1673.0 | Epsilon: 0.0 | Time: 27997.29\n",
      "Episode: 2179 | Reward: 1941.0 | Epsilon: 0.0 | Time: 28255.31\n",
      "Episode: 2199 | Reward: 1360.0 | Epsilon: 0.0 | Time: 28508.23\n",
      "Episode: 2219 | Reward: 1430.0 | Epsilon: 0.0 | Time: 28779.64\n",
      "Episode: 2239 | Reward: 828.0 | Epsilon: 0.0 | Time: 29026.13\n",
      "Episode: 2259 | Reward: 1396.0 | Epsilon: 0.0 | Time: 29282.1\n",
      "Episode: 2279 | Reward: 515.0 | Epsilon: 0.0 | Time: 29534.36\n",
      "Episode: 2299 | Reward: 1594.0 | Epsilon: 0.0 | Time: 29818.99\n",
      "Episode: 2319 | Reward: 1007.0 | Epsilon: 0.0 | Time: 30083.71\n",
      "Episode: 2339 | Reward: 923.0 | Epsilon: 0.0 | Time: 30342.38\n",
      "Episode: 2359 | Reward: 729.0 | Epsilon: 0.0 | Time: 30646.44\n",
      "Episode: 2379 | Reward: 1026.0 | Epsilon: 0.0 | Time: 30963.2\n",
      "Episode: 2399 | Reward: 1016.0 | Epsilon: 0.0 | Time: 31279.43\n",
      "Episode: 2419 | Reward: 1639.0 | Epsilon: 0.0 | Time: 31575.63\n",
      "Episode: 2439 | Reward: 1958.0 | Epsilon: 0.0 | Time: 31908.69\n",
      "Episode: 2459 | Reward: 1299.0 | Epsilon: 0.0 | Time: 32248.68\n",
      "Episode: 2479 | Reward: 1117.0 | Epsilon: 0.0 | Time: 32593.42\n",
      "Episode: 2499 | Reward: 1327.0 | Epsilon: 0.0 | Time: 32918.68\n",
      "Episode: 2519 | Reward: 1224.0 | Epsilon: 0.0 | Time: 33216.38\n",
      "Episode: 2539 | Reward: 1822.0 | Epsilon: 0.0 | Time: 33524.28\n",
      "Episode: 2559 | Reward: 1302.0 | Epsilon: 0.0 | Time: 33849.79\n",
      "Episode: 2579 | Reward: 1119.0 | Epsilon: 0.0 | Time: 34180.4\n",
      "Episode: 2599 | Reward: 918.0 | Epsilon: 0.0 | Time: 34483.88\n",
      "Episode: 2619 | Reward: 1209.0 | Epsilon: 0.0 | Time: 34805.28\n",
      "Episode: 2639 | Reward: 437.0 | Epsilon: 0.0 | Time: 35115.6\n",
      "Episode: 2659 | Reward: 1870.0 | Epsilon: 0.0 | Time: 35428.37\n",
      "Episode: 2679 | Reward: 1168.0 | Epsilon: 0.0 | Time: 35743.79\n",
      "Episode: 2699 | Reward: 1607.0 | Epsilon: 0.0 | Time: 36077.96\n",
      "Episode: 2719 | Reward: 1655.0 | Epsilon: 0.0 | Time: 36378.69\n",
      "Episode: 2739 | Reward: 1325.0 | Epsilon: 0.0 | Time: 36701.52\n",
      "Episode: 2759 | Reward: 1225.0 | Epsilon: 0.0 | Time: 36996.39\n",
      "Episode: 2779 | Reward: 1403.0 | Epsilon: 0.0 | Time: 37304.6\n",
      "Episode: 2799 | Reward: 1461.0 | Epsilon: 0.0 | Time: 37609.49\n",
      "Episode: 2819 | Reward: 1369.0 | Epsilon: 0.0 | Time: 37958.68\n",
      "Episode: 2839 | Reward: 1564.0 | Epsilon: 0.0 | Time: 38256.98\n",
      "Episode: 2859 | Reward: 476.0 | Epsilon: 0.0 | Time: 38570.64\n",
      "Episode: 2879 | Reward: 1791.0 | Epsilon: 0.0 | Time: 38933.58\n",
      "Episode: 2899 | Reward: 1858.0 | Epsilon: 0.0 | Time: 39258.24\n",
      "Episode: 2919 | Reward: 1188.0 | Epsilon: 0.0 | Time: 39631.35\n",
      "Episode: 2939 | Reward: 1435.0 | Epsilon: 0.0 | Time: 39987.44\n",
      "Episode: 2959 | Reward: 1006.0 | Epsilon: 0.0 | Time: 40316.46\n",
      "Episode: 2979 | Reward: 1325.0 | Epsilon: 0.0 | Time: 40675.13\n",
      "Episode: 2999 | Reward: 1027.0 | Epsilon: 0.0 | Time: 41003.09\n",
      "Episode: 3019 | Reward: 1478.0 | Epsilon: 0.0 | Time: 41394.05\n",
      "Episode: 3039 | Reward: 1604.0 | Epsilon: 0.0 | Time: 41713.8\n",
      "Episode: 3059 | Reward: 1373.0 | Epsilon: 0.0 | Time: 42039.4\n",
      "Episode: 3079 | Reward: 1779.0 | Epsilon: 0.0 | Time: 42378.19\n",
      "Episode: 3099 | Reward: 822.0 | Epsilon: 0.0 | Time: 42743.19\n",
      "Episode: 3119 | Reward: 970.0 | Epsilon: 0.0 | Time: 43056.63\n",
      "Episode: 3139 | Reward: 530.0 | Epsilon: 0.0 | Time: 43400.2\n",
      "Episode: 3159 | Reward: 733.0 | Epsilon: 0.0 | Time: 43725.01\n",
      "Episode: 3179 | Reward: 1421.0 | Epsilon: 0.0 | Time: 44034.8\n",
      "Episode: 3199 | Reward: 1283.0 | Epsilon: 0.0 | Time: 44335.76\n",
      "Episode: 3219 | Reward: 1291.0 | Epsilon: 0.0 | Time: 44616.09\n",
      "Episode: 3239 | Reward: 1009.0 | Epsilon: 0.0 | Time: 44922.48\n",
      "Episode: 3259 | Reward: 948.0 | Epsilon: 0.0 | Time: 45222.27\n",
      "Episode: 3279 | Reward: 965.0 | Epsilon: 0.0 | Time: 45540.5\n",
      "Episode: 3299 | Reward: 943.0 | Epsilon: 0.0 | Time: 45820.12\n",
      "Episode: 3319 | Reward: 982.0 | Epsilon: 0.0 | Time: 46111.11\n",
      "Episode: 3339 | Reward: 779.0 | Epsilon: 0.0 | Time: 46417.65\n",
      "Episode: 3359 | Reward: 1202.0 | Epsilon: 0.0 | Time: 46723.41\n",
      "Episode: 3379 | Reward: 489.0 | Epsilon: 0.0 | Time: 47034.62\n",
      "Episode: 3399 | Reward: 748.0 | Epsilon: 0.0 | Time: 47320.94\n",
      "Episode: 3419 | Reward: 1574.0 | Epsilon: 0.0 | Time: 47695.87\n",
      "Episode: 3439 | Reward: 836.0 | Epsilon: 0.0 | Time: 48003.47\n",
      "Episode: 3459 | Reward: 1040.0 | Epsilon: 0.0 | Time: 48303.07\n",
      "Episode: 3479 | Reward: 788.0 | Epsilon: 0.0 | Time: 48636.09\n",
      "Episode: 3499 | Reward: 1511.0 | Epsilon: 0.0 | Time: 48977.74\n",
      "Episode: 3519 | Reward: 1410.0 | Epsilon: 0.0 | Time: 49274.12\n",
      "Episode: 3539 | Reward: 1377.0 | Epsilon: 0.0 | Time: 49554.69\n",
      "Episode: 3559 | Reward: 262.0 | Epsilon: 0.0 | Time: 49871.48\n",
      "Episode: 3579 | Reward: 1237.0 | Epsilon: 0.0 | Time: 50239.95\n",
      "Episode: 3599 | Reward: 911.0 | Epsilon: 0.0 | Time: 50612.07\n",
      "Episode: 3619 | Reward: 1142.0 | Epsilon: 0.0 | Time: 50969.92\n",
      "Episode: 3639 | Reward: 807.0 | Epsilon: 0.0 | Time: 51275.69\n",
      "Episode: 3659 | Reward: 1732.0 | Epsilon: 0.0 | Time: 51597.01\n",
      "Episode: 3679 | Reward: 790.0 | Epsilon: 0.0 | Time: 51897.67\n",
      "Episode: 3699 | Reward: 1565.0 | Epsilon: 0.0 | Time: 52212.73\n",
      "Episode: 3719 | Reward: 1592.0 | Epsilon: 0.0 | Time: 52503.86\n",
      "Episode: 3739 | Reward: 1520.0 | Epsilon: 0.0 | Time: 52831.19\n",
      "Episode: 3759 | Reward: 1437.0 | Epsilon: 0.0 | Time: 53196.48\n",
      "Episode: 3779 | Reward: 607.0 | Epsilon: 0.0 | Time: 53533.31\n",
      "Episode: 3799 | Reward: 1102.0 | Epsilon: 0.0 | Time: 53840.87\n",
      "Episode: 3819 | Reward: 752.0 | Epsilon: 0.0 | Time: 54142.72\n",
      "Episode: 3839 | Reward: 1357.0 | Epsilon: 0.0 | Time: 54444.77\n",
      "Episode: 3859 | Reward: 531.0 | Epsilon: 0.0 | Time: 54765.34\n",
      "Episode: 3879 | Reward: 1284.0 | Epsilon: 0.0 | Time: 55050.62\n",
      "Episode: 3899 | Reward: 1405.0 | Epsilon: 0.0 | Time: 55324.34\n",
      "Episode: 3919 | Reward: 985.0 | Epsilon: 0.0 | Time: 55625.48\n",
      "Episode: 3939 | Reward: 1716.0 | Epsilon: 0.0 | Time: 55926.53\n",
      "Episode: 3959 | Reward: 1287.0 | Epsilon: 0.0 | Time: 56237.32\n",
      "Episode: 3979 | Reward: 1013.0 | Epsilon: 0.0 | Time: 56541.48\n",
      "Episode: 3999 | Reward: 1159.0 | Epsilon: 0.0 | Time: 56844.08\n",
      "Episode: 4019 | Reward: 1140.0 | Epsilon: 0.0 | Time: 57152.25\n",
      "Episode: 4039 | Reward: 966.0 | Epsilon: 0.0 | Time: 57431.78\n",
      "Episode: 4059 | Reward: 965.0 | Epsilon: 0.0 | Time: 57748.53\n",
      "Episode: 4079 | Reward: 1362.0 | Epsilon: 0.0 | Time: 58066.43\n",
      "Episode: 4099 | Reward: 1454.0 | Epsilon: 0.0 | Time: 58349.06\n",
      "Episode: 4119 | Reward: 1053.0 | Epsilon: 0.0 | Time: 58643.25\n",
      "Episode: 4139 | Reward: 1262.0 | Epsilon: 0.0 | Time: 58939.84\n",
      "Episode: 4159 | Reward: 1055.0 | Epsilon: 0.0 | Time: 59247.48\n",
      "Episode: 4179 | Reward: 2116.0 | Epsilon: 0.0 | Time: 59549.36\n",
      "Episode: 4199 | Reward: 1007.0 | Epsilon: 0.0 | Time: 59871.08\n",
      "Episode: 4219 | Reward: 186.0 | Epsilon: 0.0 | Time: 60203.22\n",
      "Episode: 4239 | Reward: 1308.0 | Epsilon: 0.0 | Time: 60531.15\n",
      "Episode: 4259 | Reward: 1481.0 | Epsilon: 0.0 | Time: 60813.35\n",
      "Episode: 4279 | Reward: 418.0 | Epsilon: 0.0 | Time: 61134.61\n",
      "Episode: 4299 | Reward: 712.0 | Epsilon: 0.0 | Time: 61422.4\n",
      "Episode: 4319 | Reward: 969.0 | Epsilon: 0.0 | Time: 61766.59\n",
      "Episode: 4339 | Reward: 562.0 | Epsilon: 0.0 | Time: 62093.02\n",
      "Episode: 4359 | Reward: 1362.0 | Epsilon: 0.0 | Time: 62418.13\n",
      "Episode: 4379 | Reward: 1411.0 | Epsilon: 0.0 | Time: 62790.57\n",
      "Episode: 4399 | Reward: 1556.0 | Epsilon: 0.0 | Time: 63119.3\n",
      "Episode: 4419 | Reward: 1715.0 | Epsilon: 0.0 | Time: 63444.04\n",
      "Episode: 4439 | Reward: 408.0 | Epsilon: 0.0 | Time: 63784.99\n",
      "Episode: 4459 | Reward: 1246.0 | Epsilon: 0.0 | Time: 64081.64\n",
      "Episode: 4479 | Reward: 1654.0 | Epsilon: 0.0 | Time: 64373.14\n",
      "Episode: 4499 | Reward: 1090.0 | Epsilon: 0.0 | Time: 64667.78\n",
      "Episode: 4519 | Reward: 487.0 | Epsilon: 0.0 | Time: 64970.86\n",
      "Episode: 4539 | Reward: 1540.0 | Epsilon: 0.0 | Time: 65304.42\n",
      "Episode: 4559 | Reward: 1386.0 | Epsilon: 0.0 | Time: 65625.71\n",
      "Episode: 4579 | Reward: 942.0 | Epsilon: 0.0 | Time: 65976.33\n",
      "Episode: 4599 | Reward: 1774.0 | Epsilon: 0.0 | Time: 66277.91\n",
      "Episode: 4619 | Reward: 1868.0 | Epsilon: 0.0 | Time: 66589.69\n",
      "Episode: 4639 | Reward: 1227.0 | Epsilon: 0.0 | Time: 66895.82\n",
      "Episode: 4659 | Reward: 940.0 | Epsilon: 0.0 | Time: 67259.61\n",
      "Episode: 4679 | Reward: 1437.0 | Epsilon: 0.0 | Time: 67605.32\n",
      "Episode: 4699 | Reward: 916.0 | Epsilon: 0.0 | Time: 68024.54\n",
      "Episode: 4719 | Reward: 546.0 | Epsilon: 0.0 | Time: 68407.23\n",
      "Episode: 4739 | Reward: 1377.0 | Epsilon: 0.0 | Time: 68824.08\n",
      "Episode: 4759 | Reward: 1328.0 | Epsilon: 0.0 | Time: 69175.03\n",
      "Episode: 4779 | Reward: 968.0 | Epsilon: 0.0 | Time: 69546.11\n",
      "Episode: 4799 | Reward: 656.0 | Epsilon: 0.0 | Time: 69888.79\n",
      "Episode: 4819 | Reward: 1418.0 | Epsilon: 0.0 | Time: 70219.12\n",
      "Episode: 4839 | Reward: 1285.0 | Epsilon: 0.0 | Time: 70535.94\n",
      "Episode: 4859 | Reward: 850.0 | Epsilon: 0.0 | Time: 70904.58\n",
      "Episode: 4879 | Reward: 1395.0 | Epsilon: 0.0 | Time: 71312.98\n",
      "Episode: 4899 | Reward: 1494.0 | Epsilon: 0.0 | Time: 71652.33\n",
      "Episode: 4919 | Reward: 1468.0 | Epsilon: 0.0 | Time: 72029.97\n",
      "Episode: 4939 | Reward: 1131.0 | Epsilon: 0.0 | Time: 72411.59\n",
      "Episode: 4959 | Reward: 860.0 | Epsilon: 0.0 | Time: 72792.38\n",
      "Episode: 4979 | Reward: 1007.0 | Epsilon: 0.0 | Time: 73139.99\n",
      "Episode: 4999 | Reward: 644.0 | Epsilon: 0.0 | Time: 73567.64\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for episode in range(Episodes):\n",
    "    # Write code here    \n",
    "    \n",
    "    # Call all the initialised variables of the environment\n",
    "    action_space = copy.deepcopy(env.action_space)\n",
    "    state_space = copy.deepcopy(env.state_space)\n",
    "    curr_state = env.state_init\n",
    "    action_size = len(action_space)\n",
    "    state_size = len(env.state_encod_arch1(env.state_init))\n",
    "    terminal_state = False\n",
    "    episode_time = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    #Call the DQN agent\n",
    "    agent = DQNAgent(state_size,action_size)\n",
    "    \n",
    "    while not terminal_state:\n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        action = agent.get_action(curr_state)\n",
    "#         print(action)\n",
    "        \n",
    "        # 2. Evaluate your reward and next state\n",
    "        reward = env.reward_func(curr_state, action, Time_matrix)\n",
    "        next_state,time_lapsed = env.next_state_func(curr_state, action, Time_matrix)\n",
    "        \n",
    "        episode_time += time_lapsed\n",
    "        \n",
    "        if episode_time > 720:\n",
    "            terminal_state = True\n",
    "        else:\n",
    "            # 3. Append the experience <s,a,s',r,t> to the memory \n",
    "            agent.append_sample(curr_state, action, reward, next_state, terminal_state)\n",
    "\n",
    "            # 4. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            \n",
    "            # 5. Keep a track of rewards, Q-values, loss\n",
    "            total_reward += reward\n",
    "            next_state = curr_state\n",
    "\n",
    "    #TRACKING Q-VALUES      \n",
    "    if ((episode+1) % 20) == 0:\n",
    "        save_total_rewards(total_reward,episode)\n",
    "        for s in States_track.keys():\n",
    "            state_encoded = np.array(env.state_encod_arch1(s)).reshape(1, 36)\n",
    "            qval = agent.model.predict(state_encoded)\n",
    "            save_tracking_states(s, qval)\n",
    "        save_obj(Rewards_track,'Reward_tracked')\n",
    "        save_obj(States_track,'States_tracked')\n",
    "        #Save Model Weights\n",
    "        agent.save('model_weights')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    if ((episode+1) % 20) == 0:   \n",
    "        print(\"Episode: \"+str(episode)+\" | Reward: \"+str(total_reward)+\" | Epsilon: \"+str(round(agent.epsilon,2)) \n",
    "             +\" | Time: \"+str(round(elapsed_time,2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "with open('models/v4/Reward_tracked.h5', 'rb') as handle:\n",
    "    Rewards = pickle.load(handle)\n",
    "with open('models/v4/States_tracked.h5', 'rb') as handle:\n",
    "    States_track = pickle.load(handle)    \n",
    "# with open('model_weights.h5', 'rb') as handle:\n",
    "#     model_weights = pickle.load(handle)   \n",
    "    \n",
    "print(len(Rewards))\n",
    "print(len(States_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Rewards['Episode'], Rewards['Rewards'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(States_track[(0, 7, 4)][(0, 0)])\n",
    "plt.show()\n",
    "# for i in States_track[(0, 7, 4)]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(States_track[(0, 7, 4)][(1,0)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,500)\n",
    "epsilon = []\n",
    "for i in range(0,500):\n",
    "    epsilon.append(0.001 + (1 - 0.001) * np.exp(-0.009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
